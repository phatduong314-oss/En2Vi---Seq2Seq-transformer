{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b3c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Thêm thư mục gốc vào path để Python nhìn thấy thư mục 'src'\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.dataset import IELTSTranslationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58d75d",
   "metadata": {},
   "source": [
    "## Load 100 dòng dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6815c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wingery\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-vi\"\n",
    "dataset_handler = IELTSTranslationDataset(model_checkpoint, sample_size = 300)\n",
    "tokenized_datasets, tokenizer = dataset_handler.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33e78fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 30\n",
      "    })\n",
      "}), MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-vi', vocab_size=53685, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t53684: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "print(dataset_handler.load_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41678e0",
   "metadata": {},
   "source": [
    "#datasethandler\n",
    "Quản lý toàn bộ quá trình xử lý dataset cho bài toán dịch máy\n",
    "Load dataset từ Hugging Face (PhoMT English-Vietnamese)\n",
    "Tokenize (chuyển text → số)\n",
    "Chuẩn bị dữ liệu cho training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dcc5b8",
   "metadata": {},
   "source": [
    "## Xem dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66082031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs (Dạng số model sẽ đọc):\n",
      "[9504, 34721, 6, 2772, 1726, 8, 29, 820, 104, 2772, 64, 4153, 2, 0]...\n",
      "Attention Mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "Labels (Target IDs):\n",
      "[43696, 232, 1602, 6, 20, 16, 48, 179, 123, 110, 115, 275, 2, 0]...\n",
      "--------------------------------------------------\n",
      "Dogfighting? i couldn't believe what i was hearing.\n",
      "Câu Tiếng Anh (Input): Dogfighting? i couldn't believe what i was hearing.\n",
      "Câu Tiếng Việt (Label): Chọi chó? tôi không thể tin điều mình đang nghe.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Lấy ngẫu nhiên 1 mẫu\n",
    "sample = tokenized_datasets['train'][random.randint(0, len(tokenized_datasets['train'])-1)]\n",
    "\n",
    "print(f\"Input IDs (Dạng số model sẽ đọc):\\n{sample['input_ids'][:20]}...\") \n",
    "print(f\"Attention Mask:\\n{sample['attention_mask'][:20]}...\")\n",
    "print(f\"Labels (Target IDs):\\n{sample['labels'][:20]}...\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Decode ngược lại thành chữ để con người đọc hiểu\n",
    "# Lưu ý: Labels thường có giá trị -100 để ignore loss, cần lọc bỏ khi decode\n",
    "decoded_input = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(decoded_input)\n",
    "labels_cleaned = [l if l != -100 else tokenizer.pad_token_id for l in sample['labels']]\n",
    "decoded_label = tokenizer.decode(labels_cleaned, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Câu Tiếng Anh (Input): {decoded_input}\")\n",
    "print(f\"Câu Tiếng Việt (Label): {decoded_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
